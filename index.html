<!DOCTYPE html>
<html>

<head lang="en">
    <meta charset="UTF-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">

    <title>InSpaceType: A Dataset and Analysis Tool for Space Type in Indoor Monocular Depth Estimation</title>

    <meta name="description" content="">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <!-- <base href="/"> -->

        <!--FACEBOOK-->
    <meta property="og:image" content="https://choyingw.github.io/works/DistDepth/img/teaser.png">
    <meta property="og:image:type" content="image/png">
    <meta property="og:image:width" content="682">
    <meta property="og:image:height" content="682">
    <meta property="og:type" content="website" />
    <meta property="og:url" content="https://choyingw.github.io/works/DistDepth/index.html"/>
    <meta property="og:title" content="DistDepth" />
    <meta property="og:description" content="DistDepth" />



<!--     <link rel="apple-touch-icon" href="apple-touch-icon.png"> -->
  <!-- <link rel="icon" type="image/png" href="img/seal_icon.png"> -->
    <!-- Place favicon.ico in the root directory -->

    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap.min.css">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.css">
    <link rel="stylesheet" href="css/app.css">

    <link rel="stylesheet" href="css/bootstrap.min.css">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/js/bootstrap.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/1.5.3/clipboard.min.js"></script>
    
    <script src="js/app.js"></script>
</head>

<body>
    <div class="row">
        <h2 class="col-md-12 text-center">
            InSpaceType: A Dataset and Analysis Tool for Space Type in Indoor Monocular Depth Estimation<br> 
            <small>
                In submission landing page
            </small>
        </h2>
    </div>
        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    * Code and evalution tools release: <a href="https://github.com/DepthComputation/InSpaceType_Benchmark">Here</a></a> 
                    <!-- * Arxiv paper: <a href="https://arxiv.org/abs/2309.13516">Here</a>
                    * Workshop version: <a href="https://openreview.net/forum?id=SYz0lN3n0H">Here</a>
                    * Supplementary Material: <a href="https://drive.google.com/file/d/1KO2xJ7e9WoSdeiEkBXDAQwjZnuSKvNXm/view?usp=sharing">Here</a> -->
                </h3>

            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    * Data</a> 
                </h3>
                <p class="text-justify">
                    <a href="https://drive.google.com/file/d/1ePsiverqYofCwuZJv98tLPWSj8bNU3ne/view?usp=sharing">[Sample data]</a>: This contains 167MB sample data<br>
                    <a href="https://drive.google.com/file/d/1d3DiLPVEEk-hRvhaEfSK6adu5DPBdlF-/view?usp=sharing">[InSpaceType Eval set]</a>: This contains 1260 RGBD pairs for evaluation use about 11.5G. For evaluation please go to our codebase<br>
                    <a href="https://drive.google.com/drive/folders/1EjdInytpvYWBT3BmQIDsTzFyP0dngP1U?usp=sharing">[InSpaceType all data]</a>:This contains 40K RGBD pairs, about 500G the whole InSpaceType dataset. The whole data is split into 8 chunks. Please download all chunks in the folder and extract them.<br>
                </p>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    * TL;DR 
                </h3>
                <p class="text-justify">
                    A new dataset and benchmark are presented that consider a crucial but often ignored facet- space type. We study 13 SOTA works to unveil underlying imbalance and assess 4 training sets to discover bias to prompt discussion on synthetic data curation.
                </p> 

            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    * Abstract
                </h3>
                <p class="text-justify">
                    Indoor monocular depth estimation helps home automation, including robot navigation or AR/VR for surrounding perception. Most previous methods primarily experiment with the NYUv2 Dataset and concentrate on the overall performance in their evaluation. However, their robustness and generalization to diversely unseen types or categories for indoor spaces (spaces types) have yet to be discovered. Researchers may empirically find degraded performance in a released pretrained model on custom data or less-frequent types. This paper studies the common but easily overlooked factor- space type and realizes a model's performance variances across spaces. We present InSpaceType Dataset, a high-quality RGBD dataset for general indoor scenes, and benchmark 13 recent state-of-the-art methods on InSpaceType. Our examination shows that most of them suffer from performance imbalance between head and tailed types, and some top methods are even more severe. The work reveals and analyzes underlying bias in detail for transparency and robustness. We extend the analysis to a total of 4 datasets and discuss the best practice in synthetic data curation for training indoor monocular depth. Further, dataset ablation is conducted to find out the key factor in generalization. This work marks the first in-depth investigation of performance variances across space types and, more importantly, releases useful tools, including datasets and codes, to closely examine your pretrained depth models.
                </p>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    * Analysis I-II [Benchmark on overall performance and space type breakdown]
                </h3>
                <p style="text-align:center;">
                    <br>
                    <h4>InSpaceType benchmark overall performance. The best number is in bold, and the second-best is underlined.</h4>
                    <image src="pics/overall_3.png" height="50px" class="img-responsive">

                    <br>                    
                    <h4>The tables study top methods among those trained only on NYUv2 for depth estimation only (N-only): MIM, PixelFormer, and among those pretrained on multiple datasets or learned from large-scale pertaining (M&LS-Pre) then finetuned on NYUv2: ZoeDepth, VPD, DepthAnything, Unidepth. Beside the breakdown, we also list easy and hard types for each method.<br></h4>
                    <image src="pics/breakdown_3.png" height="50px" class="img-responsive">
                    <image src="pics/breakdown_4.png" height="50px" class="img-responsive">
                    <image src="pics/breakdown_5.png" height="50px" class="img-responsive">
    
                
                </p>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    * Analysis III [More training datasets]
                </h3>
                <p style="text-align:center;">
                    <h4>Space type breakdown and characteristics for SimSIN, UniSIN, and Hypersim Dataset.</h4>
                    <image src="pics/training_2.png" height="100px" class="img-responsive">

                </p>
            </div>
        </div>


        <!-- <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    * Analysis IV [Cross-group generalization]
                </h3>
                <p style="text-align:center;">
                    <h4> Cross-group generalization evaluation. G1$\to$ specifies a training group (G), and each row below shows evaluation groups. Another three ranges, close, medium, and far, are used to show a breakdown by different ranges.</h4>
                    <image src="pics/range.png" height="45px" class="img-responsive">
		    <image src="pics/improve.png" height="45px" class="img-responsive">
                </p>
            </div>
        </div> -->

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    * Conclusion
                </h3>
                <p style="text-align:center;">
                    <br>
                    <h4> The work pioneers studying space types in indoor monocular depth for practical purposes, especially with the advent of many large models, but the evaluation and quality assessment still primarily focus on a single and older benchmark. First, we present novel InSpaceType Dataset that meets the high-resolution and high-quality RGBD data requirements for cutting-edge applications in AR/VR displays and indoor robotics. Previous works focusing on methods may overlook performance variances. We use InSpaceType to study 13 recent high-performing methods and analyze their zero-shot cross-dataset performance for both overall results and performance variances across space types. Even some top methods have severe imbalance, and some methods are actually less imbalanced than higher-performing ones.<br>

                        We extend our analysis to more synthetic and real datasets, including SimSIN, UniSIN, and Hypersim, to reveal their bias and guide proper usage. Especially, current synthetic data curation may not faithfully reflect the real-world high complexity in cluttered and small objects, and we suggest the best practice. Further, they may miss some common types like hallway if rendering single 3D CAD spaces separately. We further do ablation on InSpaceType and find space scale is the key factor that hinders generalization. As part of our contribution, our released tools for both research and practical aids, including codes and datasets, can diagnose a pretrained model and show its hierarchical performance breakdown. Overall, this work underscores the importance of considering performance variances in the practical deployment of models, a crucial aspect often overlooked in the field.</h4>
                </p>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">

                    <h3>Sample heirarchy labeling and breakdown:<br></h3>
                    <image src="pics/hier_table.png" height="1000px">
                </p>
            </div>
        </div>


        <div class="row">
            <div class="col-md-8 col-md-offset-2">
            The website template was borrowed from <a href="http://mgharbi.com/">Michaël Gharbi</a> 
            </div>
        </div>

    </div>
    
</body>
</html>
