<!DOCTYPE html>
<html>

<head lang="en">
    <meta charset="UTF-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">

    <title>InSpaceType: A Dataset and Analysis Tool for Space Type in Indoor Monocular Depth Estimation</title>

    <meta name="description" content="">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <!-- <base href="/"> -->

        <!--FACEBOOK-->
    <meta property="og:image" content="https://choyingw.github.io/works/DistDepth/img/teaser.png">
    <meta property="og:image:type" content="image/png">
    <meta property="og:image:width" content="682">
    <meta property="og:image:height" content="682">
    <meta property="og:type" content="website" />
    <meta property="og:url" content="https://choyingw.github.io/works/DistDepth/index.html"/>
    <meta property="og:title" content="DistDepth" />
    <meta property="og:description" content="DistDepth" />



<!--     <link rel="apple-touch-icon" href="apple-touch-icon.png"> -->
  <!-- <link rel="icon" type="image/png" href="img/seal_icon.png"> -->
    <!-- Place favicon.ico in the root directory -->

    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap.min.css">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.css">
    <link rel="stylesheet" href="css/app.css">

    <link rel="stylesheet" href="css/bootstrap.min.css">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/js/bootstrap.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/1.5.3/clipboard.min.js"></script>
    
    <script src="js/app.js"></script>
</head>

<body>
    <div class="row">
        <h2 class="col-md-12 text-center">
            InSpaceType: A Dataset and Analysis Tool for Space Type in Indoor Monocular Depth Estimation<br> 
            <small>
                In submission landing page
            </small>
        </h2>
    </div>
        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    * Code and evalution tools release: <a href="https://github.com/DepthComputation/InSpaceType_Benchmark">Here</a></a> 
                    * Arxiv paper: <a href="https://arxiv.org/abs/2309.13516">Here</a>
                    * Workshop version: <a href="https://openreview.net/forum?id=SYz0lN3n0H">Here</a>
                    * Supplementary Material: <a href="https://drive.google.com/file/d/1KO2xJ7e9WoSdeiEkBXDAQwjZnuSKvNXm/view?usp=sharing">Here</a>
                </h3>

            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    * Data</a> 
                </h3>
                <p class="text-justify">
                    <a href="https://drive.google.com/file/d/1ePsiverqYofCwuZJv98tLPWSj8bNU3ne/view?usp=sharing">[Sample data]</a>: This contains 167MB sample data<br>
                    <a href="https://drive.google.com/file/d/1d3DiLPVEEk-hRvhaEfSK6adu5DPBdlF-/view?usp=sharing">[InSpaceType Eval set]</a>: This contains 1260 RGBD pairs for evaluation use about 11.5G. For evaluation please go to our codebase<br>
                    <a href="https://drive.google.com/drive/folders/1EjdInytpvYWBT3BmQIDsTzFyP0dngP1U?usp=sharing">[InSpaceType all data]</a>:This contains 40K RGBD pairs, about 500G the whole InSpaceType dataset. The whole data is split into 8 chunks. Please download all chunks in the folder and extract them.<br>
                </p>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    * TL;DR 
                </h3>
                <p class="text-justify">
                    This work introduces a dataset and benchmark that reconsiders an important but usually overlooked factor- space type. We detailedly analyze 12 SOTA models and four popular training dataset to unveil their potential biases. Further, we study cross-type generalization and domain generalization techniques. 
                </p> 

            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    * Abstract
                </h3>
                <p class="text-justify">
                    Indoor monocular depth estimation attracted higher research interest in indoor robots to aid navigation and perception. Most previous methods primarily experiment with NYUv2 Dataset and concentrate on the overall evaluation performance. However, little is known regarding robustness and generalization in the real world where highly varying and diverse functional \textit{space types}, such as library or kitchen, are present as tailed types. This work studies the common but easily overlooked factor- space type and realizes a model's performance variance. We present InSpaceType Dataset, a high-quality and high-resolution RGBD dataset for general indoor environments. We study 12 recent methods on InSpaceType and find most of them severely suffer from performance imbalance between head and tailed types and reveal their underlying bias. We extend analysis to total 4 datasets and organize their characteristics to enlighten further research directions on proper usage of them. Further, we study interplays between types and generalization to unseen spaces. Our work marks the first in-depth investigation of performance variance across space types and, more importantly, releases useful tools, including datasets and codes, to closely examine a given pretrained model.
                </p>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    * Analysis I-II [Benchmark on overall performance and space type breakdown]
                </h3>
                <p style="text-align:center;">
                    <br>
                    <h4>InSpaceType benchmark: overall performance. The best number is in bold, and the second-best (by method) is underlined. $^*$ denotes self-supervised learning.</h4>
                    <image src="pics/overall_2.png" height="50px" class="img-responsive">

                    <br>                    
                    <h4>First table studies three top methods among those trained only on NYUv2 for depth estimation only (N-Depth only): MIM, PixelFormer, and NeWCRFs. Second table studies three top methods among those \textit{pretrained on multiple datasets or learned from large-scale pertaining (M\&LS-Pre) then finetuned on NYUv2: ZoeDepth and VPD. Beside the breakdown, we also list top-5 space types based on lower/higher error (RMSE) and accuracy ($\delta_1$). Easy and hard types are listed based on co-occurrence in the top-5 list.<br></h4>
                    <image src="pics/breakdown_1.png" height="50px" class="img-responsive">
		    <image src="pics/breakdown_2s.png" height="50px" class="img-responsive">
    
                
                </p>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    * Analysis III [More training datasets]
                </h3>
                <p style="text-align:center;">
                    <h4>Space type breakdown and characteristics for SimSIN, UniSIN, and Hypersim Dataset.</h4>
                    <image src="pics/training.png" height="100px" class="img-responsive">
                    <image src="pics/training_list.png" height="45px" class="img-responsive">

                </p>
            </div>
        </div>


        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    * Analysis IV [Cross-group generalization]
                </h3>
                <p style="text-align:center;">
                    <h4> Cross-group generalization evaluation. G1$\to$ specifies a training group (G), and each row below shows evaluation groups. Another three ranges, close, medium, and far, are used to show a breakdown by different ranges.</h4>
                    <image src="pics/range.png" height="45px" class="img-responsive">
		    <image src="pics/improve.png" height="45px" class="img-responsive">
                </p>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    * Conclusion
                </h3>
                <p style="text-align:center;">
                    <br>
                    <h4> We are the first to consider space types in indoor monocular depth estimation. We point out limitations in previous works where performance variances across types are overlooked and then present a novel dataset, InSpaceType, along with a hierarchical space type definition to facilitate our study. Twelve recent high-performing methods are examined by cross-dataset evaluation, including overall performance and space type breakdown. They more or less suffer from performance imbalance between space types, and we find that strategies like mixed-dataset pretraining or learning from large-scale pretraining show better robustness. Our analysis investigates a total of 4 training datasets and organizes their characteristics to better guide future research direction using these datasets. In particular, we find current synthetic data curation cannot faithfully reflect the real-world high complexity on cluttered and small objects in the near field. Going in-depth analysis of interplay between space types, we show generalization between groups. Specifically, generalization to different depth ranges is harder than to scene appearance and arrangement change. We further inspect three approaches, class reweighting, type-balanced sampler, and meta-learning to enhance the generalization. <br>

Overall, this work pursues a practical purpose and emphasizes the importance of this usually overlooked factor- space type in indoor environments. We release the analysis tool, including codes and datasets, to diagnose a pretrained model and show metrics for hierarchical space type breakdown. We believe such a tool is useful to broad interest in the robotic vision community. Note that the work focuses on monocular depth estimation as it is fundamental and useful in wide applications such as indoor robots or AR/VR. Multiview reconstruction may also need attention on space type as a future study.</h4>
                </p>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">

                    <h3>Sample heirarchy labeling and breakdown:<br></h3>
                    <image src="pics/hier_table.png" height="1000px">
                </p>
            </div>
        </div>


        <div class="row">
            <div class="col-md-8 col-md-offset-2">
            The website template was borrowed from <a href="http://mgharbi.com/">Michaël Gharbi</a> 
            </div>
        </div>

    </div>
    
</body>
</html>
